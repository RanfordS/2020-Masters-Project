\chapter{Introduction to Python}

Python is a general-purpose programming language designed by Guido van Rossum,
with an emphasis on readability and reusability \citep{vanRossum:1996:Foreword}.
It comes with an extensive standard library and is one of the most popular
programming languages.

There are multiple options for interacting with Python, these include:
\begin{itemize}
    \item typing commands into an interpreter,
    \item writing files and running them with an interpreter,
    \item using an online service such as Google Colab.
\end{itemize}
A small snippet of trivial python code is given below to display the syntax.
{\singlespacing
\inputminted[
    frame=lines,
    framesep=2mm,
    mathescape,
    linenos
    ]{python}{Sample.py}
}

\section{Neural Networks in Python}

Before using any neural network packages, a few small examples networks were
produced in python.
The \texttt{numpy} package was used to perform matrix operations,
and the \texttt{matplotlib.pyplot} package was used for plotting,
as these features were non-trivial.
For some examples, the \texttt{keras} module from the \texttt{tensorflow}
package was also used to access specific datasets, but their broader purpose
will be explored in the next section.

\subsection{Single Layer Boston Housing Data}

The first example network consisted of 3 input neurons connected to a
single output neuron, which is given by the equation
\begin{align*}
    y &= \tanh\left(b + \sum_{i=1}^{3} w_ix_i\right).
\end{align*}
The bias term was implemented by adding a forth input node with a constant value
of one, giving
\begin{align*}
    y &= \tanh\left(\mathbf{w}\cdot\mathbf{x}\right),
\end{align*}
where $w_4 = b$, and $x_4 = 1$.

The network was trained using the Boston housing dataset from \texttt{keras},
which provided a number of attributes about houses from late 1970's Boston
suburbs.
The network took in normalised data from three of these attributes (number of
rooms, highway accessibility index, percentage of lower status population), and
used them to predict the value of the house.

Training was performed using backpropagation, defined by the equation
\begin{align*}
    \Delta w_i &= \eta\Rpdiff{e}{w_i},\\
    d &= y - y_t,\\
    e &= \frac{1}{2}d^2,
\end{align*}
where $e$ is the network error, $y$ is the network prediction, and $y_t$ is the
target value.
By definition,
\begin{align*}
    y &= \tanh(\text{net}),\\
    \text{net} &= \sum w_ix_i.
\end{align*}
By chain rule,
\begin{align*}
    \Rpdiff{e}{w_i} &=
    \Rpdiff{e}{y} \cdot \Rpdiff{y}{\text{net}} \cdot \Rpdiff{\text{net}}{w_i}.\\
    %
    \Rpdiff{net}{w_i} &= x_i,\\
    \Rpdiff{y}{net} &= 1 - \tanh^2(net) = 1 - y^2,\\
    \Rpdiff{e}{y} &= \frac{1}{2}\Rpdiff{(y - y_t)^2}{y} = y - y_t = d,\\
    %
    \therefore\Rpdiff{e}{w_i} &=
    x_i \cdot (1 - y^2) \cdot d.
\end{align*}
$\Delta w_i$ can be written in vector notation, giving
\begin{align*}
    \Delta \mathbf{w} &= \eta\cdot\mathbf{x}\cdot (1-y^2)\cdot d.
\end{align*}
Weights were updated after each sample using
\begin{align*}
    \mathbf{w}' &= \mathbf{w} - \Delta\mathbf{w},
\end{align*}
where $\mathbf{w}'$ is the new set of weights.

The network was initialised using random weights, and was trained using the full
dataset.
\rfig{\input{DiagramBHPySingle.tex}}{%
    Graph of network weights against iteration number.
}
After 10 epochs, the mean square error had reduced from 0.1637 to 0.0507.
Note that the weight value lines appear to be jagged, this was a side effect of
updating the value after each individual presentation; a problem which can be
mitigated by batching $\Delta\mathbf{w}$ terms from multiple presentations.



\subsection{Multilayer Boston Housing Data}

The same task was repeated using the full set of attributes.
To accommodate this, a larger network with 13 input neurons, 1 output neuron,
and $n$ hidden neurons; which was expressed by the equation
\begin{align*}
    h_i &= \tanh\left(b_i + \sum_{j=0}^{13} w_{i,j}x_j\right),\\
    y &= b + \sum_{i=1}^{n} w_i h_i.
\end{align*}
Similar to the single layer example, a constant neuron was added to the input
and hidden layer to implement the bias, giving
\begin{align*}
    \mathbf{h} &= \tanh(W\mathbf{x}),\\
    \mathbf{h}' &= \begin{pmatrix} \mathbf{h} \\ 1 \end{pmatrix},\\
    y &= \mathbf{w}\cdot\mathbf{h}',
\end{align*}
where $\tanh$ acts component-wise on the input.

All of the inputs were batched together into a single matrix $X$, where each
column was a data point, giving
\begin{align*}
    \Phi &= \tanh(W\cdot X),\\
    \Psi &= \begin{pmatrix} \Phi \\ \mathbf{1} \end{pmatrix},\\
    \mathbf{y} &= \mathbf{w}\cdot\Psi,
\end{align*}
where $\mathbf{y}$ is a row vector of results.
The error gradients for the output neurons were given by
\begin{align*}
    \mathbf{d} &= \mathbf{y} - \mathbf{y}_t,\\
    e &= \frac{1}{2}\left|\mathbf{d}\right|^2,\\
    \mathbf{g}_O &= \Rpdiff{e}{\mathbf{w}} = \mathbf{d}\cdot\Psi^T;
\end{align*}
and for the hidden neurons by
\begin{align*}
    D &= \hat{\mathbf{w}}^T\cdot\mathbf{d},\\
    G_H &= ((1 - \Phi\odot\Phi)\odot D) \cdot X^T,
\end{align*}
where $\odot$ is the component-wise product, and $\hat{\mathbf{w}}$ is the
weight vector without the bias term.
Note that $\hat{\mathbf{w}}^T$ and $\mathbf{e}$ are column and row vectors
respectively, and that their product is a matrix.
See Appendix \ref{app:BHMDeriv} for full derivation.
\rfig{\input{DiagramBHPyMulti.tex}}{%
    Graph of network error against iteration number for
    various numbers of hidden neurons using the Boston housing data.
}
The network was trained multiple times with varying numbers of hidden neurons.
The same random seed value was used for all trials.
In each case, the network successfully reduced it's error to roughly the same
level:
for 1 neuron, from 0.1670 to 0.0380;
for 2 neurons, from 0.1799 to 0.0390; and
for 3 neurons, from 0.1757 to 0.0396.
The difference between the results was negligible, which suggested that one
hidden neuron was sufficient for learning the data.

When compared with the single layer network from the previous section, the
single neuron, multilayer network performed better (0.0507 vs 0.0380) for two
reasons:
\begin{enumerate}
    \item it had access to all 13 attributes, instead of the 3 attributes that
        had been manually selected; and
    \item the additional layer enabled it to apply a linear transformation to
        the activation.
\end{enumerate}
Additionally, the use of batched inputs resulted in a smoother descent, as the
total gradient is descended; instead of multiple, often opposing gradients.



\subsection{Logical XOR}

Using the same, multilayer network architecture, a network was trained to
perform the logical exclusive-or (XOR) operation.
The input matrix $X$ contained all four combinations of binary inputs, and the
target outputs in $\mathbf{y}_t$.
\begin{align*}
    X &= \begin{pmatrix}
        0 & 1 & 0 & 1 \\
        0 & 0 & 1 & 1 \\
        1 & 1 & 1 & 1
    \end{pmatrix},\\
    \mathbf{y}_t &= \begin{pmatrix}
        0 & 1 & 1 & 0
    \end{pmatrix}.
\end{align*}
Given that the problem is mathematically well defined, a successfully trained
network should reduce the error to near zero.
Training the network for varying numbers of hidden neurons gave the following
results.
\rfig{\input{DiagramXORPy.tex}}{%
    Graph of network error against iteration number for
    various numbers of hidden neurons using the XOR data.
}
The final mean square errors for one, two, and three hidden neurons were 0.1713,
$2.04\times10^{-5}$, and $9.65\times10^{-6}$ respectively.
These results showed that a single, nonlinear neuron is not sufficient for
learning the XOR problem, as proven by \cite{Minsky:1969:Perceptrons}, and is
only capable of solving three of the four inputs at a time.
Two nonlinear neurons is sufficient for solving the problem.

It is important to note that the learning rates were much more susceptible to
the initial weights than that of the Boston housing data.
Under certain initial conditions, the network displayed long periods of
negligible change before significant learning occurred, the longest of which
that had been observed lasted over 800 iterations.
Error spiking was observed across the majority of initial conditions for both
two and three hidden neurons.



\section{Using TensorFlow and Keras}

Creating efficient neural networks by hand was difficult, repetitive, and prone
to mistakes; and making simple modifications, such as changing the activation
function, could prove tricky for larger networks.
Thankfully, python packages that automate large portions of the process are
available, namely TensorFlow.

Once installed, TensorFlow can be imported into python.
TensorFlow contains a module called Keras, which provides a number of objects
and constructors that make the process much simpler.
The XOR network, for instance, can be constructed with a few lines of code.
\begin{minted}[
    frame=lines,
    framesep=2mm,
    mathescape,
    linenos
    ]{python}
import tensorflow as tf
layers = tf.keras.layers
# Create a model
model = tf.keras.models.Sequential()
# Add a dense hidden layer with the built in $\tanh$ activation function
model.add(layers.Dense(3, input_dim=2, activation='tanh')
# Add the output layer with linear activation
model.add(layers.Dense(1))
# Finalise the model and specify the loss function
model.compile(loss='mean_squared_error')
\end{minted}
TensorFlow provides multiple ways of constructing models, and a wide variety of
options to configure layers, models, and optimisers, as well as custom
definitions.

TensorFlow also supports multithreading and GPU acceleration, making it
especially suitable for large networks and training sets.



\subsection{Boston Housing}

The Boston housing example was repeated using TensorFlow, with two hidden
layers, each with five neurons using the $\tanh$ activation function, and a
linear output layer, to see if adding additional layers and neurons would
improve the results.
The data was also split into two groups, training and validation, the latter of
which was not used for training the network.
For a well-fitting network, the loss values for both groups should be similar,
and the ratio between them provided a rough measure of over-fitting.

The network was trained over 2000 epochs using the stochastic gradient descent
optimiser, obtaining an final training loss of 0.0421, and validation loss of
0.0396.
The ratio between the loss values is $\sim0.94$, this value is close to 1,
suggesting that the network was not over-fitting.
\rfig{\input{DiagramBHTFSGD.tex}}{%
    Graph of training and validation loss against iteration
    number with ``SGD'' optimiser.
}
\newpage
The network was trained again using the ``Adam'' optimiser
\citep{Kingma:2014:Adam}, which uses estimations of both first and second-order
moments, and consistently outperforms the stochastic gradient descent method.

With the Adam optimiser, the final training and validation loss values were
0.0147 and 0.0443 respectively.
Although the loss value is smaller than with stochastic gradient descent, the
loss ratio was $\sim3.01$, which suggests that the network was over-fitting.
This was further evidenced the loss graph, which show that beyond a certain
point, the training loss and validation loss diverge, with the validation loss
increasing.
As such, the network is expected to be less generalised.
\rfig{\input{DiagramBHTFAdam.tex}}{%
    Graph of training and validation loss against iteration
    number with ``Adam'' optimiser.
}

%2000 epochs

%SGD [5,5]
%initial loss 0.228166446089744570
%  final loss 0.042430859059095380
%initial vali 0.214338675141334530
%  final vali 0.046398963779211044
% vali / loss 1.093519311

%SGD [3,3]
%initial loss 0.285416126251220700
%  final loss 0.033157549798488620
%initial vali 0.329692572355270400
%  final vali 0.045487113296985626
% vali / loss

%SGD [3]
%initial loss 0.494845181703567500
%  final loss 0.041440241038799286
%initial vali 0.568243682384491000
%  final vali 0.048694450408220290

%Adams [5,5]
%initial loss 0.425099462270736700
%  final loss 0.014745806343853474
%initial vali 0.409027785062789900
%  final vali 0.044336102902889250

\subsection{Linear Regression}

\rfig{\input{DiagramRegLinear.tex}}{%
    Graph showing the data points and network's corresponding line of best fit.
}
%initial loss 0.62278741598129270000
%  final loss 0.00022495193115901202

\subsection{Character Recognition}

\rfig{\input{DiagramCRResults.tex}}{%
    Hand written numbers from the MNIST validation set, the network prediction,
    and the certainty of the network.
}
%500 epochs
%initial loss: 2.3656811714172363
%  final loss: 0.06899717450141907
%initial vali: 2.1969106197357178
%  final loss: 0.10314085334539413
%  training size:   60000
%validation size:   10000
%  training errors: 4223
%validation errors: 1247
%  training %err:   7.038333333333333
%validation %err:   12.47

%Convol: 50 epochs, batch=600
%initial loss: 0.5744658349454403
%initial vali: 0.2477983083575964
%final loss:   0.010112296480219812
%final loss:   0.05892044545849785
%training size:     60000
%training errors:   482
%training %err:     0.8033333333333333
%validation size:   10000
%validation errors: 493
%validation %err:   4.93

