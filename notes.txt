wiki summary (https://en.wikipedia.org/wiki/Artificial_neural_network)

computing systems inspired by biological neural networks
image recognition
	large set of manually labeled data
	no prior knowledge of what constitutes the label
	auto' generates identifying characteristics from data
collection of connected artificial neurons (nodes)
	receive signal from previous layer (edges)
	take weighted sum with bias
	non-linear function
	output new signal to next layer
applications
	computer vision
	speech recognition
	machine translation
	social network filtering
	playing games
	medical diagnosis
confirmations are empirical, not theoretical

history

1943: Warren McCulloch and Walter Pitts, created a computational model for neural networks [3] (Difficult paper)
1949: D. O. Hebb, Learning hypothesis based on neural plasticity (Hebbian learning) [5]
1954: Farley and Wesley A. Clark, first to simulate a Hebbian Network [6]
1958: Rosenblatt, perceptron [7,8]
1960: Henry J. Kelley, control theory backpropagation [15]
1961: Arthur Earl Bryson Jr, backpropagation [16]
1965: Ivakhnenko and Lapa, first functional with many layers [9,10,11]
1969: Marvin Lee Minsk and Seymour Aubrey Papert, limit of perceptron [21]
1970: Seppo Linnainmaa, automatic differentiation [17,18]
1973: Stuart Dreyfus, adapt parameters in proportion to error gradients [19]
1975: Paul John Werbos, pratical multi-layer network training [?]
1982: Paul John Werbos, applied Linnainmaa's AD method to neural networks [12,20]
1992: John (Juyang) Weng, Narendra Ahuja and Thomas S. Huang, max-pooling (CNN) [22,23,24]
1992: J端rgen Schmidhuber, multi-level hierarchy of networks pre-trained unsupervised, backpropagation [25]
2006: Geoffrey Hinton et al, learning proposition using successive layers with restricted Boltzmann machine [27]
2010: Ciresan and colleagues, GPU backpropagation feasibility [29,30]
2012: Andrew Yan-Tak Ng and Jeffrey Adgate Dean, high-level concepts without labels [28]



Recurrent Neural Network

can use their internal memory to process variable length inputs

LSTM (https://en.wikipedia.org/wiki/Long_short-term_memory)

RNN architecture
feedback connections
can process entire sequences of data
applications
	connected handwriting recognition
	speech recognition
	anomaly detection
	time series predictions
components
	cell
		remembers values over arbitrary time interval
	input gate
	output gate
	forget gate
partially solves the vanishing gradient problem

Recursive

apply the same set of weights recursively
structured input, structured prediction, variable-size input structure
input traversed in topological order
applications
	natural language processing

history

1997: Sepp Hochreiter and J端rgen Schmidhuber, constant error carousel units [1,5]
1999: Felix Gers, J端rgen Schmidhuber and Fred Cummins, forget gate introduced [5,6]
2000: Felix Gers, J端rgen Schmidhuber and Fred Cummins, peephole connections [5,7]
2014: Kyunghyun et al, simplified variant [8]



Autoencoder

network learns to copy the input to the output
encoding section is narrower than the input/output

	x       x'
	    u
	y       y'
	    v
	z       z'

sparse autoencoder
	improve performance on classification by encouraging sparsity
	hidden layer may have more nodes than input, but only a small number may be active at a given time (dropout)



Deep Belief Network

probabilistically reconstruction inputs
acts as feature detector



Vanishing Gradient Problem

affects gradient-based learning methods and backpropagation
caused by taking the product of many small values
prevents value from changing, partially due to rounding errors
solutions
	multi-level heirarchy
		pre-trained one level at a time
		fine-tuned using back propagation



Reinforcement Learning [An Introduction to Deep Reinforcement Learning]

concerned with software agents actions for cumulative rewards
balance between exploration and exploitation
no assumption of knowledge
agent's performance compared with target, difference in performance gives rise to "regret"
must reason about long term performance



Unsupervised Learning

finds previously unknown patterns within a data set
allows the modelling of probability densities of given inputs
	cluster analysis to group/segment attributes
	anomaly detection
	autoencoders
	deep belief network
		probabilistically reconstruction inputs
		acts as feature detector



Supervised Learning

learning of a function that maps an input to an output
function inferd from examples
ideally, the algorithm will correctly label unseen instances

issues
	inductive bias
	overfitting

