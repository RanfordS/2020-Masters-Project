\chapter{Reinforcement Learning}

The neural network structures previously discussed for the Boston Housing data
are essentially curve fitting models, with continuous inputs and output.
The character recognition networks for the MNIST dataset can also be though of
as an abstract curve fitting model with multiple outputs, where the result is
a probability value.

In both cases, training is performed on a set of inputs with known target
outputs.

Not all problems can be constructed in this manner as some problems require
discrete actions to be taken.
Similarly, problems may contain reward systems.
Such systems may have delayed rewards, were one action provides an immediate
reward, but prevents reaching a state of higher reward.

Consider the ``Chain'' problem from \cite{Strens:2000:ABF}:
\rfig{\input{DiagramRLChain.tex}}{%
    Five-state chain problem.
    Arrows from the top of the states denote action 0, and from the bottom
    denote action 1.
    The labels on each arrow denote the reward associated with that action.
    The system is initially at state 0.
}
The optimal, long term strategy for this system is to always perform action 0,
which will cause the system to reach state 4 and obtain a reward of 10 on each
step.
A greedy algorithm will discover the reward from always performing action 1,
which will cause the system to stay at state 0 and obtain a reward of 1 on each
step.
The greedy algorithm is only optimal for four steps or less.



\section{Q-Learning}

One way to find a solution to this problem without neural networks is
``Q-Learning''.
Proposed by \cite{Watkins:1989:Learning}, Q-Learning is an algorithm for finding
optimal solutions to Markovian domain problems, such as the chain.

The algorithm starts with a table, referred to as the Q-table, which has a row
for each system state, and a column for each action, initialised as zero.
Once trained, each element of the Q-table will represent the maximum expected
reward for each action in each state.

Let $Q(s,a)$ denote the Q-table value for state $s$ and action $a$.

Initially, actions are taken at random, which allows the agent to explore the
network; but as training progresses, the agent gradually shifts towards making
decisions based on the Q-table values.

Each time an action $a$ is taken from state $s$, the reward $r$ and new state
$s'$ are observed, and the Q-table is updated using the Bellman equation:
\begin{align}
    Q(s,a) \leftarrow Q(s,a) + \alpha (r + \gamma\max_{a'}Q(s',a') - Q(s,a)),
    \label{eq:RL:QL}
\end{align}
where $\alpha$ is the learning rate, and $\gamma$ is the discount rate, which is
needed to prevent the values from growing exponentially.
Note that $\max_{a'}Q(s',a')$ is the maximum expected future reward of state
$s'$, according to the current Q-table values.

The algorithm was applied to the chain problem with learning rate $\alpha =
0.8$, and discount rate $\gamma = 0.7$, for 200 episodes, each consisting of 50
steps.
During training, the probability of the agent performing a random action was
given by $0.01^{t/E}$, where $t$ is the episode number, and $E$ in the total
number of episodes.
\rfig{\input{DiagramChainQLearn.tex}}{%
    Graph of score against episode number for the Q-Learning chain problem.
}
Once trained, the agent was able to obtain the highest possible score of 460.
The final values of the Q-table are provided below.
\rfig{%
    \begin{tabular}{c|c c}
        State & Action 0 & Action 1\\
        \hline
        0 &  8.00 & 6.60\\
        1 & 11.43 & 6.60\\
        2 & 16.33 & 6.60\\
        3 & 23.33 & 6.60\\
        4 & 33.33 & 6.60\\
    \end{tabular}}{%
    Q-table for the full trained Q-Learning chain problem.
}
Note that all of the values in the Action 0 column are larger than the
corresponding values in the Action 1 column, which denotes the larger expected
future reward.



\section{Deep Q-Learning}

The Q-Learning algorithm works well for problems with well defined states and
actions, but cannot accommodate problems where the state is either to too large,
not directly visible, or non-Markovian.
To solve this problem, a neural network may be trained to take in a set of
observations and predict the corresponding Q-values for each action
\citep{Lin:1990:Self}.

To prevent the system from forgetting past experiences, a replay memory was
created, which stores a limited number of experiences \citep{Lin:1992:Self}.
Each experience contains: the initial observations, the action performed,
the reward received, and the new observations.

The use of replay memory also allows training to be performed with fewer
total experiences.

After each action, the experience is added to the replay memory.
If the memory is full, the oldest element is removed to accommodate the newest.
In python, this behaviour is handled automatically by using the \texttt{deque}
class, from the inbuilt \texttt{collections} package.

Initially, a number of random trails are performed to fill the memory.

Training is then performed in a similar manner as Q-Learning.
As the Q-values cannot be modified directly, a batch of random experiences from
the replay memory are selected for training the network.
The network target is calculated using a modified version of Equation
\ref{eq:RL:QL}, where $Q(s,a)$ is the network's Q-value prediction.

Deep Q-Learning was applied to the chain problem using a single-layer network
with: five input neurons, one for each state; and two densely connected, ReLU
output neuron.
Training was performed over 40 episodes, each consisting of 50 steps, with the
same parameters as before.
After each step, the network trained using a random sample of 100 experiences
from a replay memory with a capacity of 500.
\rfig{\input{DiagramChainDeepQLearn.tex}}{%
    Graph of score against episode number for the Deep Q-Learning chain problem.
}



\subsection{Prioritised Experience Replay}

The replay memory provided a simple way for the network to remember and reuse
past experiences.
In the first instance, experiences were randomly sampled from the memory with a
uniform distribution.

\cite{Schaul:2015:Prioritized}, notes that improved training performance can be
obtained by prioritising some experiences over others.
This is achieved by creating a weighted distribution that favours elements with
higher priority values.
The probability of picking the $i^\text{th}$ memory is given by
\begin{align*}
    P(i) = \frac{p_i^a}{\sum_{k} p_k^a},
\end{align*}
where $p_i$ is the priority value of the $i^\text{th}$ memory, and $a$ is a
parameter that controls the distribution.
Using $a = 0$ will make the distribution uniform.

The priority value is defined as
\begin{align*}
    p_t = \left|\delta_t\right| + e,
\end{align*}
where $e$ is some constant to prevent zeros, and $\delta_t$ is the temporal
difference,
\begin{align*}
    \delta_t = r + \gamma\max_{a'}Q(s',a') - Q(s,a).
\end{align*}

However, a problem arises from using this distribution, which is that the sample
distribution no longer matches the source distribution, which in turn causes
bias.
To compensate, a weighting is introduced to the network training loss:
\begin{align*}
    \left(\frac{1}{N}\cdot\frac{1}{P(i)}\right)^{\beta},
\end{align*}
where $\beta$ is a parameter to be increased during training.

This technique is known as Prioritised Experience Replay (PER).

Efficient sampling of the replay memory according to the priority values was
achieved by implementing a ``sum tree'', which replaces the \texttt{deque}
structure, see Appendix \ref{app:SumTree} for details.

Deep Q-Learning with PER was applied to the chain problem using the same
network layout as before.
Using PER, the agent could be reliably trained using fewer episodes.
Training was performed over 30 episodes, each consisting of 50 steps, with the
same memory capacity, batch size, and parameters as before.
%\rfig{\input{DiagramChainPERDeepQLearn.tex}}{%
%    Graph of score against episode number for the Deep Q-Learning chain problem
%    with Prioritised Experience Replay.
%}



\subsection{Fixed Q-Targets}

When training, the expected future reward from state $s'$ is calculated using
the same network as the expected future reward from state $s$, which is being
updated each step.
This causes both $Q(s',a')$ and $Q(s,a)$ to change on each step, which can cause
instabilities.

To reduce the instability, a separate target network may be introduced.
This gives
\begin{align*}
    \delta_t = r + \gamma\max_{a'}Q^T(s',a') - Q^P(s,a),
\end{align*}
where $Q^T$ is the target network, and $Q^P$ is the policy network.
Both networks are initialised with the same weights, but only the policy network
is trained, hence providing a fixed approximation of the future Q-values.

The weights of the target network are periodically updated to match those of the
policy network, this is done every $\tau$ steps, where $\tau$ is some chosen
parameter.

Deep Q-Learning with Fixed Q-Targets was applied to the chain problem using the
same network layout as before.
Training was reliably performed over 35 episodes, each consisting of 50 steps,
with $\tau = 10$, and the same memory capacity, batch size, and parameters as
before.



\subsection{Double Deep Q-Networks}

When dealing with larger systems, the random agent may not have fully explored
the system.
In such cases, there may be insufficient information about which actions should
be taken during the initial stages of training.
%During the initial parts of training there is not enough information about which
%actions should be taken.
Taking the action with the maximum Q-value can lead to false positives, which
can cause over predictions for frequently taken actions.

The solution proposed by \cite{Hasselt:2010:Double}, is to use two Q-Networks,
$Q^A$ and $Q^B$ to predict the Q-Values, which reduces overestimation within the
policy.
At each step, one network is randomly chosen for training, and the other is used
to predict the maximum future value, giving either:
\begin{align*}
    \delta_t^A &= r + \gamma Q^B(s',a*) - Q^A(s,a),\\
    a* &= \underset{a}{\operatorname{argmax}}\ Q^A(s',a);
\end{align*}
or
\begin{align*}
    \delta_t^B &= r + \gamma Q^A(s',a*) - Q^B(s,a),\\
    a* &= \underset{a}{\operatorname{argmax}}\ Q^B(s',a).
\end{align*}
Doing this reduces the overestimation of the Q-values and improves training
stability.

For the trivial case of the chain problem, applying Double Deep Q-Networks
negatively impacts performance, and often prevented the network from learning
the optimal policy.
%A possible explanation for the reduced training performance is that the network
%does not suffer the problems which Double Deep Q-Networks attempt to solve, and
%so separating the training across two networks only slows the training.
A possible reason for the reduced training performance is that the network does
not suffer from overestimation when learning the chain problem, and so applying
Double Deep Q-Networks to the problem causes underestimation, which in turn
prevents the policy from progressing through the chain.



\subsection{Dueling Deep Q-Network}

When recalling the interpretation of the Q-value, \cite{Wang:2015:Dueling} noted
that the Q-value represents a combination of both the state values and the state
dependent action advantages, and proposed an architecture that explicitly
separates the two.
This might be naively implemented as $Q(s,a) = V(s) + A(s,a)$, but
\citeauthor{Wang:2015:Dueling} noted that this does not produce unique values
for $V(s)$ and $A(s,a)$.
To counteract this, the $A(s,a)$ value are offset by a property thereof, such as
the maximum or mean value, the latter giving
\begin{align}
    Q(s,a) = V(s) + \left(A(s,a) - \underset{a'}{\operatorname{mean}}A(s,a')\right).
    \label{eq:RL:Dueling}
\end{align}
Implementing this into the neural network is achieved as follows:
\begin{enumerate}
    \item a number of layers input the observations and produce a set of state
        representations;
    \item the state representation is inputted into two sets of layer streams,
        the former predicts the single state value, and the latter predicts the
        advantage values for each action;
    \item the two streams are merged according to Equation \ref{eq:RL:Dueling},
        producing a Q-value for each action.
\end{enumerate}


Dueling Deep Q-Learning was applied to the chain problem using a network with
five input neurons, one for each state of the environment; which is connected
to two streams: a single densely connected ReLU for the state value, and two
densely connected ReLUs for the advantage values which feed into a ``lambda''
layer, which calculates the offset values; and an add layer, which provides the
output Q-value.


Training was reliably performed over 30 episodes, each consisting of 50 steps,
with the same memory capacity, batch size, and parameters as before.



\subsection{Combined}

All of the previously discussed improvements can be used simultaneously, in any
combination, to enhance the training performance of a policy.

A Dueling Deep Q-Network with PER and Fixed Q-Targets was applied to the chain
problem.
Training was reliably performed over 25 episodes, each consisting of 50 steps,
with the same memory capacity, batch size, and parameters as before.



\section{Policy Gradient}

Although Q-Learning provided an efficient method to produce deterministic
polices, many problems require stochastic policy, where actions are selected
according to probabilities.
Another issue with Q-Learning is that it uses an implicit greedy algorithm,
where the action with the highest q-value is always selected; small changes in
the q-value can radically change the policy.

A method for producing stochastic polices was introduced by
\cite{Sutton:2000:Policy}, which defined probability of taking action $a$ at
state $s$ as $\pi(s,a,\theta)$, where $\pi$ is a general function approximator,
such as a neural network, and $\theta$ a vector of parameters for $\pi$.

It is necessary to calculate the state distribution, which is given by
\begin{align*}
    d^\pi(s) = \frac{N(s)}{\sum_{s'} N(s')},
\end{align*}
where $N(s)$ is the number of occurrences of state $s$.

One of two possible objectives may be chosen, either:
maximise the cumulative discounted reward from a designated initial state,
$s_0$, or;
maximise the long-term expected reward per step.
In either case, two functions are defined:
$\rho(\pi)$, which is the long-term performance metric;
and $Q^\pi(s,a)$, which is the value of a given state-action pair.

In the case of cumulative discounted reward, the long-term performance is given
by
\begin{align*}
    \rho(\pi) = E\left\{ \sum_{i=0}^{\infty} \gamma^{i}r_{i}
    \Rgiven s_0,\, \pi \right\}
\end{align*}
and the state-action pair value is given by
\begin{align*}
    Q^\pi(s,a) = E\left\{ \sum_{i=0}^{\infty} \gamma^{i}r_{t+i}
    \Rgiven s_t=s,\, a_t=a,\, \pi \right\},
\end{align*}
where $\gamma$ is the discount rate, and $r_i$ is the reward at step $i$.

In the case of expected reward per step, the long-term performance is given by
\begin{align*}
    \rho(\pi) = \sum_s d^\pi(s) \sum_a \pi(s,a) R_s^a,
\end{align*}
where $R_s^a$ is the reward for taking action $a$ at state $s$;
and the state-action pair value is given by
\begin{align*}
    Q^\pi(s,a) = \sum_{t=1}^{\infty} E\left\{ r_t - \rho(\pi)
    \Rgiven s_t=s,\, a_t=a,\, \pi \right\}.
\end{align*}

In either case, the policy gradient is given by
\begin{align*}
    \Rpdiff{\rho}{\theta} = \sum_s d^\pi(s) \sum_a \Rpdiff{\pi(s,a)}{\theta} Q^\pi(s,a),
\end{align*}
which can be discretised to give an iterative formula for learning.

Unlike Q-Learning, training is performed at the end of an episode, not during;
as such, a replay memory cannot be applied.
