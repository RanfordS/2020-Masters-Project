\chapter{Introduction to Python}

Python is a general-purpose programming language designed by Guido van Rossum,
with an emphasis on readability and reusability \citep{vanRossum:1996:Foreword}.
It comes with an extensive standard library and is one of the most popular
programming languages.

There are multiple options for interacting with Python, these include:
\begin{itemize}
    \item typing commands into an interpreter,
    \item writing files and running them with an interpreter,
    \item using an online service such as Google Colab.
\end{itemize}
A small snippet of trivial python code is given below to display the syntax.
{\singlespacing
\inputminted[
    frame=lines,
    framesep=2mm,
    mathescape,
    linenos
    ]{python}{Sample.py}
}

\section{Neural Networks in Python}

Before using any neural network packages, a few small examples networks were
produced in python.
The \texttt{numpy} package was used to perform matrix operations,
and the \texttt{matplotlib.pyplot} package was used for plotting,
as these features were non-trivial.
For some examples, the \texttt{keras} module from the \texttt{tensorflow}
package was also used to access specific datasets, but their broader purpose
will be explored in the next section.

\subsection{Single Layer Boston Housing Data}

The first example network consisted of 3 input neurons connected to a
single output neuron, which is given by the equation
\begin{align*}
    y &= \tanh\left(b + \sum_{i=1}^{3} w_ix_i\right).
\end{align*}
The bias term was implemented by adding a forth input node with a constant value
of one, giving
\begin{align*}
    y &= \tanh\left(\mathbf{w}\cdot\mathbf{x}\right),
\end{align*}
where $w_4 = b$, and $x_4 = 1$.

The network was trained using the Boston housing dataset from \texttt{keras},
which provided a number of attributes about houses from late 1970's Boston
suburbs.
The network took in normalised data from three of these attributes (number of
rooms, highway accessibility index, percentage of lower status population), and
used them to predict the value of the house.

Training was performed using backpropagation, defined by the equation
\begin{align*}
    \Delta w_i &= -\eta\Rpdiff{\text{err}^2}{w_i},\\
    \text{err} &= y_p - y_t,
\end{align*}
where $y_p$ is the network prediction, and $y_t$ is the target value.
By definition,
\begin{align*}
    y_p &= \tanh(\text{net}),\\
    \text{net} &= \sum w_ix_i.
\end{align*}
By chain rule,
\begin{align*}
    \Rpdiff{\text{err}}{w_i} &=
    \Rpdiff{\text{net}}{w_i} \cdot
    \Rpdiff{y_p}{\text{net}} \cdot
    \Rpdiff{\text{err}^2}{y_p}.\\
    %
    \Rpdiff{net}{w_i} &= x_i,\\
    \Rpdiff{y_p}{net} &= 1 - \tanh^2(net) = 1 - y_p^2,\\
    \Rpdiff{\text{err}^2}{y_p} &= \Rpdiff{(y_p - y_t)^2}{y_p}
    = 2(y_p - y_t) = 2\text{err},\\
    %
    \therefore\Rpdiff{\text{err}^2}{w_i} &=
    x_i \cdot (1 - y_k^2) \cdot 2\cdot\text{err}.
\end{align*}
The factor of 2 can be absorbed by the $\eta$ term, and $\Delta w_i$ can be
written in vector notation, giving
\begin{align*}
    \Delta \mathbf{w} &= -\eta\cdot\mathbf{x}\cdot (1-y_k^2)\cdot \text{err}.
\end{align*}
Weights were updated after each sample using
\begin{align*}
    \mathbf{w}' &= \mathbf{w} + \Delta\mathbf{w},
\end{align*}
where $\mathbf{w}'$ is the new set of weights.

The network was initialised using random weights, and was trained using the full
dataset, over ten epochs.

\begin{center}
    \includegraphics[width=0.7\textwidth]{../Code/BostonHousingResult.pdf}
    \captionof{figure}{Graph of network weights against iteration number.}
\end{center}


\subsection{Multilayer Boston Housing Data}

\subsection{Logical XOR}

\section{Using TensorFlow and Keras}

\subsection{Boston Housing Data}

\subsection{Logical XOR}

\subsection{Linear Regression}

\subsection{Character Recognition}
