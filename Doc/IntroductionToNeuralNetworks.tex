\chapter{Introduction to Neural Networks}

%\TODO{Chapter: Introduction}

\section{Biological Neurons}

Biological neurons are electrically excitable cells that are found in almost all
animals.
These neurons can transmit and receive electrical signals to one another via
synaptic connections, which maybe either excitatory or inhibitory.
Any given neuron will be either active or inactive depending on whether or not
its input exceeds a threshold.
\rfig{\input{DiagramNeuron.tex}}{%
    Diagram of a biological neuron.
}
Signals are received by the neuron via connections to dendrites and soma.
If the threshold is met, electrical signals are sent along the axon to the
terminal, where it is connected to other neurons, or to a controllable cell such
as a neuromuscular junction.

%\TODO{Section: Biological Neurons}



\section{Artificial Intelligence}

The idea of artificial beings capable of human intelligence can be traced back
to mythical stories from ancient Greece.
One such story was that of a mythical automaton called Talos, who circled an
island's shores to protect it from pirates and other invaders.

In the $19^\text{th}$ century, other notions of artificial intelligence were
explored by fiction in stories such as Mary Shelley's ``Frankenstein'', and
Karel \v{C}apek's ``R.U.R.''.
Some of the fictional writings of the $20^\text{th}$ century further continued
exploring the concept in novels such as Isaac Asimov's ``I, Robot''.



Academic research into artificial intelligence began around the 1940's,
primarily due to findings in neurological research at the time.
The first explorations of artificial neural networks was done by
\cite{McCulloch:1943:Logical}, who investigated how simple logic functions
might be performed by idealised networks.
The neurons within these networks operated using some basic logic rules, applied
to a discrete time system which, can be summarised using the expression
\begin{align*}
    N(t) &= (E_1(t-1) \vee E_2(t-1) \vee \dots)
        \wedge \neg(I_1(t-1) \vee I_2(t-1) \vee \dots),
\end{align*}
where $N(t)$ is the state of a neuron at time $t$, and $E_i(t-1)$ and $I_i(t-1)$
are the states of the excitatory and inhibitory connections from the previous
time step respectively.
The result is such that the neuron will only be active if at least one
excitatory connection is active and all inhibitory connections are inactive.
The versatility of this definition is demonstrated in the following examples.
\rfig{%
    \begin{tabular}{ccc}
        SR Flip-flop & & AND Gate\\
        \input{DiagramMcculSRFlipFlop.tex} & & \input{DiagramMcculAND.tex}\\
        $\displaystyle N_M(t+1) = (N_S(t) \vee N_M(t)) \wedge\neg N_R(t)$ &
        &
        $\displaystyle N_R(t+2) = N_A(t) \vee N_B(t)$\\
    \end{tabular}}{%
    Two common logic circuits using McCulloch's neurons
    where the arrows and circles indicate excitatory and inhibitory
    connections respectively.
}
While this model provided insight into the mechanisms by which neurons operate,
the structure was static, and incapable of learning.



McCulloch's work was later cited by psychologist \cite{Hebb:1949:Organization},
as important for understanding how logical operations are performed; but
proposed that the structure of biological neurons was dynamic, not static, and
that frequently repeated stimuli caused gradual development.
At the scale of neuron, it was theorised that if one neuron successfully
excited another, the connection between them would strengthen, hence
increasing the likelihood that the former would be able to excite the latter
again in the future.
His theory was supported by research conducted by himself and others that showed
that intelligence-test performance in mature patients was often unaffected by
brain operations that would have prevented development in younger patients,
which suggested that learnt stimuli are processed differently to unknown
stimuli.
This hypothesis became known as Hebbian learning.



Computer simulations applying this theory to a small network were done by
\cite{Farley:1954:Simulation}.
The actions of the network were compared to that of a servo system which must
counteract any displacements so as to maintain a steady position.
The network was trained using a set of input patterns, which were subject to
non-linear transformations.
Similar to the Hebbian theory, when the network produces the correct responses,
the active connections are strengthened.
Although the results were of little neurophysiological significance, the results
were of great use for demonstrating computational simulations, which were
considerably slower at the time.

%\TODO{Section: Artificial Intelligence}



\subsection{Perceptrons}

The idea of the perceptron was originally conceived by
\cite{Rosenblatt:1958:Perceptron}, to represent a simplified model of
intelligent systems free from particularities of biological organisms, whilst
maintaining some of their fundamental properties.

The perceptron was built as a dedicated machine that consisted of a number of
photovoltaic cells, analogous to a retina, that feed into an``association
area''.
This association area contains a number of cells that each calculate a weighted
sum of the receptor values and output a signal if it exceeds a threshold.
Expressed mathematically, the output of a given association cell is given by
\begin{align*}
    A_i &= \begin{cases}
        1, & \sum_j w_{i,j}x_j > \theta\\
        0, & \text{otherwise}
    \end{cases},
\end{align*}
where $x_j$ is the value from the $j^\text{th}$ photovoltaic cell, $w_{i,j}$
is the weight of the connection between association cell $i$ and photovoltaic
cell $j$, and $\theta$ is the threshold.
These value weights were implemented using variable resistance wires that the
perceptron could adjust automatically.
The outputs from the association area are then connected to response cells,
which operate in a similar fashion to the association cells.
The activation of these response cells are the outputs of the perceptron, and
indicated the classification of the input.

Similar to \cite{Farley:1954:Simulation}, the method by which the perceptron
adjusted it weights was also based on which cells were active, and whether the
correct output was produced; except that the perceptron was also able to
``penalise'' weights when an incorrect result was outputted.

This machine was initially trained to reliably identify three different shapes:
a square, a circle, and a triangle; and did so with a better than chance
probability.
When attempting to use the perceptron for more complicated tasks, such as
character recognition, it failed to produce better than chance results.



After a decade of unsuccessful real world application attempts, a book titled
``\citetitle{Minsky:1969:Perceptrons}'' by \cite{Minsky:1969:Perceptrons}, was
released.
The book provided a rigorous mathematical analysis of the model, the results
showed that single layered, simple linear perceptron networks could not
calculate XOR predicates.
A 2017 reissue of the book contained a foreword by L\'eon Bottou, who wrote
``Their rigorous work and brilliant technique does not make the perceptron look
very good...''



Following the book's release, perceptron research effectively halted for 15
years until the first successful uses of multilayer networks by
\cite{McClelland:1986:Parallel}, which also served as a departure from the
neuron outputs being boolean values.
The multilayered structure of this new model allowed it to calculate the XOR
predicates that the single layer perceptrons could not.

The output of the units within these networks were defined by
\begin{align*}
    \Rvec{a}(t+1) &= \Rvec{F}(\Rvec{a}(t),\Rvec{net}_1(t),\Rvec{net}_2(t),...),
\end{align*}
where $\Rvec{net}_i$ is the $i^\text{th}$ propagation rule applied to the
inputs, $\Rvec{F}$ is the activation function, and $\Rvec{a}(t)$ is the
activation of the units at time step $t$.
The model usually uses a simplified version which can be summarised as
\begin{align*}
    a_i &= F\left(\sum_j w_{i,j}o_j\right),
\end{align*}
where $o_j$ is the output of unit $j$.
Hebbian learning could be performed the network by using iterative methods,
the most simple of which was given by
\begin{align*}
    \Delta w_{i,j} &= \eta\,a_i o_j,
\end{align*}
where $\eta$ is the learning rate, which is a constant.

%\TODO{Subsection: Perceptrons}



\subsection{Backpropagation}

In order for a neural network to learn, it must undergo some form of
optimisation process.
For the perceptron, this process was one of positive and negative reinforcement.



In the field of control theory, an optimisation method known as gradient descent
was developed by \cite{Kelley:1960:Gradient}, in which a given function of the
system is either maximised or minimised.
This is achieved by taking partial derivatives of the function with respect to
each parameter, which gives an approximation of how the function value will
change as the parameter changes.
By evaluating the partial derivatives, multiplying them by a constant, and
adding them to their respective parameters, the parameter values can be updated.
Using these new parameter values, one can expect to improve the function value.
This can be written as
\begin{align*}
    w_i' &= w_i + \eta\Rpdiff{f}{w_i}(\Rvec{x}),
\end{align*}
where $f(\Rvec{x})$ is the function to be optimised, $w_i$ is a parameter of
$f$, $w_i'$ is the updated parameter, and $\eta$ is ascent/descent parameter.
Positive $\eta$ values will maximise the function value, where as negative
values will minimise it.
The magnitude of $\eta$ determines the rate at which the method will attempt
change the parameters: if the value is too large, the method will overshoot the
optimal values; if the value is too small, the method will be too slow to
converge.
This method is known as stochastic gradient descent.

When the method was applied to neural networks, researchers sometimes
encountered an issue now known as the vanishing gradient problem.
A computer program will typically calculate the gradient via repeat applications
of chain rule; if there are many small terms, the gradient will tend to zero,
and the learning rate of the network will be minimal.



One of the methods that overcame this problem was developed by
\cite{Schmidhuber:1992:Compression}, where each layer of the network was
pre-trained to predict the next input from previous inputs.
Once each layer had been pre-trained, the network was then fine tuned using
backpropagation.
The method also provided a way of calculating which inputs were least expected,
so that more training time could be devoted to learning them.

Since then, computational power has significantly increased, and the slow
convergence caused by the vanishing gradient problem is less significant.
Further more, backpropagation and a simple variant the model outlined by
\cite{McClelland:1986:Parallel}, have become the standard for neural networks.
Namely
\begin{align*}
    x_i &= \phi\left(b_i + \sum_j w_{i,j} x_i\right),
\end{align*}
where $x_i$ is the output of neuron $i$, $w_{i,j}$ is the weight of connection
from $j$ to $j$, $b_i$ is the input bias of $i$, and $\phi$ is the activation
function.

%\TODO{Subsection: Backpropagation}



\section{Other Types of Artificial Neural Networks}

The preceding discussion has been focused on densely connected neural
networks, where each neuron in a layer is connected to every neuron in the
previous, but it is important to note, that many other neural network
architectures are often used together, and maybe more suitable under certain
contexts.

%\TODO{Section: Types of Neurons}



\subsection{Convolutional Neural Networks}
\label{subsec:history:conv}

Many of the neural networks that had been employed for image recognition, such
as the preceptron, suffered from two major issues:
\begin{enumerate}
    \item processing high resolution images required each neuron in the first
        layer to be connected to every input, which caused the number of
        connections to become too large to process; and,
    \item most networks could not correctly identify an input if it was shifted.
\end{enumerate}
Similar to how biology inspired neural networks, findings in neurophysiology
inspired the architectures that would overcome these issues.
\cite{Hubel:1959:Receptive}, discovered that certain cells within a cat's
visual cortex would only respond to stimuli from specific regions of the retina.
Another important observation was that neighboring cells had overlapping
response regions.

Later research by \cite{Hubel:1962:Receptive}, also distinguished two categories
of cells termed:
``simple'', which had distinct excitatory and inhibitory connections, where
firing was maximised by slits at specific angles that passed through the centre;
and ``complex'', which could not be mapped out as trivial inhibitory/excitatory
regions, but were maximised slits at specific angles, regardless of position.



These findings inspired \cite{Fukushima:1980:Neocognitron}, to design the
neocognitron.
The neocognitron featured alternating layers of ``S-Planes'' and ``C-Planes'',
which were representations of the simple and complex cells respectively.
Each plane contains a number of feature maps, each unit within a feature map
is a function of a small region of the previous layer, a process now commonly
referred to as convolution.
S-Plane feature maps connect to all feature maps of the previous layer,
but C-Plane feature maps only connect to the corresponding feature map.
\rfig{\input{DiagramNeocognitron.tex}}{%
    Representation of the neocognitron's connectivity.
}
Each layer of the network reduces the size of input image until the final layer
consists of single unit feature maps.
The network was originally trained to distinguish 5 digits, numbers 0 to 4,
using an unsupervised learning method, and was the first to reliably handle
shifted inputs.



\cite{Waibel:1989:Phoneme}, used concepts from the neocognitron to design the
time delay neural network, which was originally proposed for phoneme
recognition.
The networks were initially trained using backpropagation to detect and
distinguish between three acoustically similar phonemes (/b/, /d/, and /g/).
The model consisted of units similar to the neocognitron's S-Planes, where the
output of a unit is a function of a region from the previous layer.
The input for the model was a 2D spectrogram of an audio sample, with each
column representing a set of 16 spectral coefficients for a given time frame.

The first hidden layer contained columns of 8 units, each of which convolved the
spectral coefficients across 3 time frames, requiring a total of 384 weights.
Similarly, the second hidden layer contained columns of 3 units, each convolving
the previous layer across 5 time frames, requiring a total of 120 weights.
Finally, the output layer contains three units, each of which is a function of
the previous layer's corresponding row sum.
The most active unit is the phoneme present in the audio.

Once trained, the network was able to detect the correct phoneme, in real time,
under a variety of contexts, with an error of 1.5\%, a significant improvement
over the 6.3\% error from the most popular method at the time.



Similar techniques were used by \cite{LeCun:1989:Backpropagation}, to classify
handwritten digits.
A large number of 16 by 16 pixel images were used to train the network using
backpropagation.
The network consisted of two convolutional layers, and two dense layers.
Although the all of the units in a feature map shared the same weights, each
had a unique, adjustable bias.
Once trained, the network correctly classified 99.86\% of the training data, and
95\% of the test data.



This technique of combining convolutional and dense dense layers was also used
by \cite{Yamaguchi:1990:Neural}, for speech recognition.
Similar to \citeauthor{Waibel:1989:Phoneme}, the network takes a 2D spectrogram
as it's input and predicts which word was spoken.

The first layer, referred to as the event-net, convolves the input spectrogram
using a two layer subnetwork, which consists of a hidden partially connected
layer, and a fully connected single unit output layer.
Backpropagation is used to train each subnetwork of the event-net to only fire
when a word of the corresponding category is inputted.

The second layer, referred to as the time-alignment procedure, performs an
operation now known as max pooling, where each unit of the layer is the largest
value from the response region. If all the values are similar, use the one from
the middle of the search range; if all of the values are very low, the response
region size is increased

The third layer, referred to as the word-net, is another convolutional layer
using subnetwork, consisting of a fully connected layer, and a full connected
single unit output layer.
Backpropagation is used again in the same manner as in the event-net.

The forth layer, referred to as the super-net, is another convolutional
subnetwork that takes $N$ inputs and densely connects to $N+1$ outputs.
Each output corresponds to a word, except the last which denotes a rejected
result.

Finally, a decision algorithm compares the two highest outputs and rejects the
result if the difference does not exceed a preset threshold.



\subsection{Recurrent Neural Networks}

So far, the progression of time has been implemented as being another spacial
dimension.
Although this methodology proved effective for phoneme recognition tasks, it did
not work for other, more advanced temporal tasks.



\cite{Jordan:1986:Serial}, noted that representing time as a spacial dimension
required the network inputs to be stored in a buffer.
This buffer storage method was susceptible to a number of problems, including:
\begin{itemize}
    \item inability to account for input errors,
    \item lack of distinction between relative positions,
    \item difficulty with repeated actions, and
    \item difficulty processing different orderings of the same actions.
\end{itemize}
He proposed an alternative architecture, where the network takes the temporal
input in serial, whilst modifying its internal state.
This internal state is implemented by introducing cycles and delays into the
network.
Any network containing one or more cycles is described as being recurrent.

Such a network was initially trained to measure 8 phonetic features across time.
The input layer contained two groups of units:
input units, providing data from the current time frame;
and state units, providing compressed data from all previous time frames.
A hidden layer connects fully to all units from both groups in the input layer.
The output layer connects fully to the hidden layer.
The state units are connected to the output units and to themselves.
\rfig{\input{DiagramJordan.tex}}{%
    Example of Jordan's recurrent neural network.
}
During training, the network was exposed to one utterance at a time, and trained
to output the corresponding measures for each time frame.
For some combinations phonemes and features, any value is acceptable, these were
marked as ``don't-care'', and did not affect the error.
Due to the recurrent nature of the network, weight values were only updated
every 4 time steps.
The network learnt to process utterances and produce feature graphs that could
be used to identify the correct phonemes.



% \cite{Robinson:1987:Utility}
% based on control theory
% letter to word conversion
%   26 input, one for each letter
%   34 hidden
%   34 state
%    8 output, one for each word
%   learnt to output word at the start of the first letter of next word
% might not include



\cite{Elman:1990:Finding}, expanded Jordan's critiques of buffer-based
techniques by noting that there was no evidence of any biological equivalent.
He proposed that the state units should correspond to the hidden layer instead.
For each unit in the hidden layer, there is a corresponding state unit, which
holds the previous value of the hidden unit.
Units in the hidden layer is connected to the corresponding state unit, and
fully to the previous layer.
\rfig{\input{DiagramElman.tex}}{%
    Example of Elman's recurrent neural network.
}
The network structure was initially used to predict sequential bit patterns.
In one such experiment, a number of random sentences were generated using a
small lexicon, and were presented to a network one character at a time without
breaks.
Each character was presented to the network as a 5-bit number, via 5 input
units.
The network processes this input using 20 hidden units each with a respective
state unit, and outputs a prediction of the next letter via 5 output units.

Once trained, the network struggled to predict the first letter of each randomly
selected word, but was able to accurately predict the letters that followed.



% \cite{Schmidhuber:1992:Learning}
% history compression
%   $o(t) = f(i(t),h(t))$, $h(t) = g(i(t-1),h(t-1))$
%   $o$ is output/prediction
%   $i$ is input
%   $h$ is hidden/internal state
%   $f$ and $g$ are functions
%\cite{Schmidhuber:1992:Learning} introduced the principle of history
%compression.
%A discrete time
%adaptive method for removing redundant information from sequences
%\begin{center}
%    \input{DiagramHistoryCompression.tex}
%    \captionof{figure}{Visualisation of history compression.}
%\end{center}



The recurrent neural networks previously described performed well for problems
with short time delays, but failed to perform tasks involving more than 10
discrete time steps.
The reason for this is the vanishing/exploding gradient problem.
Because errors backpropagate through the recurrent connections across multiple
time steps, the errors either tend towards zero or infinity.

A solution to this problem was proposed by \cite{Hochreiter:1997:Long}, where
the error term of the memory unit could be guaranteed to be a fixed constant.
This was achieved by using multiplicative gates to control the input and output
of the memory cell, resulting in a ``constant error carousel''.

Each cell took a weighted sum of the inputs and own previous state; and applied
a nonlinear function $g$ to obtain the net input signal, which connects to a
single, self-connected state unit.
A nonlinear function $h$ is applied to the value of the state unit to obtain
the output signal.
Gate units were controlled by taking a weighted sum of the layers inputs and
outputs, applying a nonlinear function, and multiplying by the relevant signal.
These gates prevented the network from perturbing its state, and prevents the
cell state from perturbing the reset of the network, allowing it to learn
long-lag-time tasks,
This new neural architecture was termed ``Long Short-Term Memory'' (LSTM).
% recurrent network architecture
% appropriate gradient-based learning algorithm
% variant of RTRL for learning



Although LSTM networks could learn long-lag-time tasks, they could not learn
certain, very long inputs that contain multiple sub-sequences.
This was because continual input streams would cause the cell state to grow
without bound, even in cases where the inputs suggest that the state should be
occasionally reset.
This problem was solved by \cite{Gers:1999:Learning}, by introducing a third
gate to the LSTM, which controlled the cell's internal recurrent connection.

It should be noted that an LSTM cell can ``learn to never forget'', hence
recovering the previous model.
% LSTM fails to learn to correctly process certain very long or continual time
% series that are not a priori segmented into appropriate training subsequences
% with clearly defined beginnings and ends
% continual input stream eventually may cause the internal values of the cells
% to grow without bound, even if the repetitive nature of the problem suggests
% they should be reset occasionally
% introduced forget gate
% formulation allows for the network to learn to never forget inputs, hence new
% model is an expansion of the previous and can solve any problem that the
% previous could



A further expansion of the model by \cite{Gers:2000:Recurrent}, added
``peephole'' connections between the internal state unit and the gates, allowing
the gates to access the state, even when the output gate was closed.
Additionally, the output function, $h$, was removed, as there was no empirical
evidence to suggest it was required.

These peephole connections allowed the model to perform count and timing tasks,
such as producing nonlinear, precisely timed spikes.
A full diagram is given in Figure \ref{INN:LSTM}

\newpage
\rfig{\input{DiagramLSTM.tex}}{%
    Visualisation of two LSTM units, where $\sigma$,
    $g$, and $h$ are function units;
    $+$ is a weighted summation unit; and $\cdot$ is multiply unit.
}\label{INN:LSTM}
% during learning no error signals are propagated back from gates
% via peephole connections



