\chapter{Introduction to Python and TensorFlow}

Python is a general-purpose programming language designed by Guido van Rossum,
with an emphasis on readability and reusability \citep{vanRossum:1996:Foreword}.
It comes with an extensive standard library and is one of the most popular
programming languages.

There are multiple options for interacting with Python, these include:
\begin{itemize}
    \item typing commands into an interpreter,
    \item writing files and running them with an interpreter,
    \item using an online service such as Google Colab.
\end{itemize}
A small snippet of trivial python code is given below to display the syntax.
{\singlespacing
\inputminted[
    frame=lines,
    framesep=2mm,
    mathescape,
    linenos
    ]{python}{Sample.py}
}

\section{Neural Networks in Python}

Before using any neural network packages, a few small examples networks were
produced in python.
The \texttt{numpy} package was used to perform matrix operations,
and the \texttt{matplotlib.pyplot} package was used for plotting,
as these features were non-trivial.
For some examples, the \texttt{keras} module from the \texttt{tensorflow}
package was also used to access specific datasets, but their broader purpose
will be explored in the next section.

\subsection{Single Layer Boston Housing Data}

The first example network consisted of 3 input neurons connected to a
single output neuron, which is given by the equation
\begin{align*}
    y &= \tanh\left(b + \sum_{i=1}^{3} w_ix_i\right).
\end{align*}
The bias term was implemented by adding a forth input node with a constant value
of one, giving
\begin{align*}
    y &= \tanh\left(\mathbf{w}\cdot\mathbf{x}\right),
\end{align*}
where $w_4 = b$, and $x_4 = 1$.

The network was trained using the Boston housing dataset from \texttt{keras},
which provided a number of attributes about houses from late 1970's Boston
suburbs.
The network took in normalised data from three of these attributes (number of
rooms, highway accessibility index, percentage of lower status population), and
used them to predict the value of the house.

Training was performed using backpropagation, defined by the equation
\begin{align*}
    \Delta w_i &= \eta\Rpdiff{e}{w_i},\\
    d &= y - y_t,\\
    e &= \frac{1}{2}d^2,
\end{align*}
where $e$ is the network error, $y$ is the network prediction, and $y_t$ is the
target value.
By definition,
\begin{align*}
    y &= \tanh(\text{net}),\\
    \text{net} &= \sum w_ix_i.
\end{align*}
By chain rule,
\begin{align*}
    \Rpdiff{e}{w_i} &=
    \Rpdiff{e}{y} \cdot \Rpdiff{y}{\text{net}} \cdot \Rpdiff{\text{net}}{w_i}.\\
    %
    \Rpdiff{net}{w_i} &= x_i,\\
    \Rpdiff{y}{net} &= 1 - \tanh^2(net) = 1 - y^2,\\
    \Rpdiff{e}{y} &= \frac{1}{2}\Rpdiff{(y - y_t)^2}{y} = y - y_t = d,\\
    %
    \therefore\Rpdiff{e}{w_i} &=
    x_i \cdot (1 - y^2) \cdot d.
\end{align*}
$\Delta w_i$ can be written in vector notation, giving
\begin{align*}
    \Delta \mathbf{w} &= \eta\cdot\mathbf{x}\cdot (1-y^2)\cdot d.
\end{align*}
Weights were updated after each sample using
\begin{align*}
    \mathbf{w}' &= \mathbf{w} - \Delta\mathbf{w},
\end{align*}
where $\mathbf{w}'$ is the new set of weights.

The network was initialised using random weights, and was trained using the full
dataset.
\rfig{\input{DiagramBHPySingle.tex}}{%
    Graph of network weights against iteration number.
}
After 10 epochs, the mean square error had reduced from 0.1637 to 0.0507.
Note that the weight value lines appear to be jagged, this was a side effect of
updating the value after each individual presentation; a problem which can be
mitigated by batching $\Delta\mathbf{w}$ terms from multiple presentations.



\subsection{Multilayer Boston Housing Data}

The same task was repeated using the full set of attributes.
To accommodate this, a larger network with 13 input neurons, 1 output neuron,
and $n$ hidden neurons; which was expressed by the equation
\begin{align*}
    h_i &= \tanh\left(b_i + \sum_{j=0}^{13} w_{i,j}x_j\right),\\
    y &= b + \sum_{i=1}^{n} w_i h_i.
\end{align*}
Similar to the single layer example, a constant neuron was added to the input
and hidden layer to implement the bias, giving
\begin{align*}
    \mathbf{h} &= \tanh(W\mathbf{x}),\\
    \mathbf{h}' &= \begin{pmatrix} \mathbf{h} \\ 1 \end{pmatrix},\\
    y &= \mathbf{w}\cdot\mathbf{h}',
\end{align*}
where $\tanh$ acts component-wise on the input.

All of the inputs were batched together into a single matrix $X$, where each
column was a data point, giving
\begin{align*}
    \Phi &= \tanh(W\cdot X),\\
    \Psi &= \begin{pmatrix} \Phi \\ \mathbf{1} \end{pmatrix},\\
    \mathbf{y} &= \mathbf{w}\cdot\Psi,
\end{align*}
where $\mathbf{y}$ is a row vector of results.
The error gradients for the output neurons were given by
\begin{align*}
    \mathbf{d} &= \mathbf{y} - \mathbf{y}_t,\\
    e &= \frac{1}{2}\left|\mathbf{d}\right|^2,\\
    \mathbf{g}_O &= \Rpdiff{e}{\mathbf{w}} = \mathbf{d}\cdot\Psi^T;
\end{align*}
and for the hidden neurons by
\begin{align*}
    D &= \hat{\mathbf{w}}^T\cdot\mathbf{d},\\
    G_H &= ((1 - \Phi\odot\Phi)\odot D) \cdot X^T,
\end{align*}
where $\odot$ is the component-wise product, and $\hat{\mathbf{w}}$ is the
weight vector without the bias term.
Note that $\hat{\mathbf{w}}^T$ and $\mathbf{e}$ are column and row vectors
respectively, and that their product is a matrix.
See Appendix \ref{app:BHMDeriv} for full derivation.
\rfig{\input{DiagramBHPyMulti.tex}}{%
    Graph of network error against iteration number for
    various numbers of hidden neurons, using the Boston housing data.
}
The network was trained multiple times with varying numbers of hidden neurons.
The same random seed value was used for all trials.
In each case, the network successfully reduced it's error to roughly the same
level:
for 1 neuron, from 0.1670 to 0.0380;
for 2 neurons, from 0.1799 to 0.0390; and
for 3 neurons, from 0.1757 to 0.0396.
The difference between the results was negligible, which suggested that one
hidden neuron was sufficient for learning the data.

When compared with the single layer network from the previous section, the
single neuron, multilayer network performed better (0.0507 vs 0.0380) for two
reasons:
\begin{enumerate}
    \item it had access to all 13 attributes, instead of the 3 attributes that
        had been manually selected; and
    \item the additional layer enabled it to apply a linear transformation to
        the activation.
\end{enumerate}
Additionally, the use of batched inputs resulted in a smoother descent, as the
total gradient is descended; instead of multiple, often opposing gradients.



\subsection{Logical XOR}

Using the same, multilayer network architecture, a network was trained to
perform the logical exclusive-or (XOR) operation.
The input matrix $X$ contained all four combinations of binary inputs, and the
target outputs in $\mathbf{y}_t$.
\begin{align*}
    X &= \begin{pmatrix}
        0 & 1 & 0 & 1 \\
        0 & 0 & 1 & 1 \\
        1 & 1 & 1 & 1
    \end{pmatrix},\\
    \mathbf{y}_t &= \begin{pmatrix}
        0 & 1 & 1 & 0
    \end{pmatrix}.
\end{align*}
Given that the problem is mathematically well defined, a successfully trained
network should reduce the error to near zero.
Training the network for varying numbers of hidden neurons gave the following
results.
\rfig{\input{DiagramXORPy.tex}}{%
    Graph of network error against iteration number for
    various numbers of hidden neurons, using the XOR data.
}
The final mean square errors for one, two, and three hidden neurons were 0.1713,
$2.04\times10^{-5}$, and $9.65\times10^{-6}$ respectively.
These results showed that a single, nonlinear neuron is not sufficient for
learning the XOR problem, as proven by \cite{Minsky:1969:Perceptrons}, and is
only capable of solving three of the four inputs at a time.
Two nonlinear neurons is sufficient for solving the problem.

It is important to note that the learning rates were much more susceptible to
the initial weights than that of the Boston housing data.
Under certain initial conditions, the network displayed long periods of
negligible change before significant learning occurred, the longest of which
that had been observed lasted over 800 iterations.
Error spiking was observed across the majority of initial conditions for both
two and three hidden neurons.



\section{Using TensorFlow and Keras}

Creating efficient neural networks by hand was difficult, repetitive, and prone
to mistakes; and making simple modifications, such as changing the activation
function, could prove tricky for larger networks.
Thankfully, python packages that automate large portions of the process are
available, namely TensorFlow.

Once installed, TensorFlow can be imported into python.
TensorFlow contains a module called Keras, which provides a number of objects
and constructors that make the process much simpler.
The XOR network, for instance, can be constructed with a few lines of code.
\begin{minted}[
    frame=lines,
    framesep=2mm,
    mathescape,
    linenos
    ]{python}
import tensorflow as tf
layers = tf.keras.layers
# Create a model
model = tf.keras.models.Sequential()
# Add a dense hidden layer with the built in $\tanh$ activation function
model.add(layers.Dense(3, input_dim=2, activation='tanh')
# Add the output layer with linear activation
model.add(layers.Dense(1))
# Finalise the model and specify the loss function
model.compile(loss='mean_squared_error')
\end{minted}
TensorFlow provides multiple ways of constructing models, and a wide variety of
options to configure layers, models, and optimisers, as well as custom
definitions.

Once constructed, the model can be trained using the \texttt{fit} method, which
provides a history of network properties from each epoch; and used to predict
inputs using the \texttt{predict} method.
Models can be saved to a file using the \texttt{save} method, which includes the
network structure and weights, and loaded using
\texttt{tensorflow.keras.models.load\_model}.

TensorFlow also supports multithreading and GPU acceleration, making it
especially suitable for large networks and training sets.



\subsection{Optimisers and the Boston Housing Data}

The Boston housing example was repeated using TensorFlow, with two hidden
layers, each with five neurons using the $\tanh$ activation function, and a
linear output layer, to see if adding additional layers and neurons would
improve the results.
The data was also split into two groups, training and validation, the latter of
which was not used for training the network.
For a well-fitting network, the loss values for both groups should be similar,
and the ratio between them provided a rough measure of over-fitting.
\rfig{\input{DiagramBHTFSGD.tex}}{%
    Graph of training and validation loss against iteration number for the
    Boston housing data using SGD.
}
%\newpage
The network was trained over 2000 epochs using the stochastic gradient descent
optimiser, obtaining an final training loss of 0.0421, and validation loss of
0.0396.
The ratio between the loss values is $\sim0.94$, this value is close to 1,
suggesting that the network was not over-fitting.

The network was trained again using the ``Adam'' optimiser
\citep{Kingma:2014:Adam}, which uses estimations of both first and second-order
moments, and consistently outperforms the stochastic gradient descent method.

With the Adam optimiser, the final training and validation loss values were
0.0147 and 0.0443 respectively.
Although the loss value is smaller than with stochastic gradient descent, the
loss ratio was $\sim3.01$, which suggests that the network was over-fitting.
This was further evidenced the loss graph, which show that beyond a certain
point, the training loss and validation loss diverge, with the validation loss
increasing.
As such, the network is expected to be less generalised.
\rfig{\input{DiagramBHTFAdam.tex}}{%
    Graph of training and validation loss against iteration number for the
    Boston housing data using Adam.
}
When comparing the two optimisers, Adam trains the network significantly faster
than SGD.
Given a sufficient number of epochs, stochastic gradient descent will over-fit
the data, just as it did with Adam.
\newpage\noindent
TensorFlow provided eight optimisers at the time writing:
\begin{enumerate}

    \item\textsc{SGD}, stochastic gradient descent, uses a first-order error
        approximation;

    \item\textsc{Adam}, uses a first and second-order error approximation
        \citep{Kingma:2014:Adam};

    \item\textsc{AdaMax}, Adam variant based on the infinity norm
        \citep{Kingma:2014:Adam};

    \item\textsc{AdaGrad}, uses parameter-specific learning rates based on
        update frequency \citep{Duchi:2011:Adagrad};

    \item\textsc{AdaDelta}, stochastic gradient descent with adaptive learning
        rates \citep{Zeiler:2012:Adadelta};

    \item\textsc{RMSProp}, uses a root-mean-squared approach to adjust the
        learning rate \citep{Hinton:2014:RMSProp}.

    \item\textsc{NAdam}, Adam variant using Nesterov momentum
        \citep{Dozat:2016:NAdam}.

    \item\textsc{FTRL}, follow the regularized leader, online learning
        algorithm \citep{McMahan:2013:FTRL}.

\end{enumerate}
The optimal choice of optimiser varies with the network and data set size.
Adaptive rate optimisers, such as Adagrad, perform best with sparse input data;
but Adam, Adamax, NAdam, and RMSProp are typically the best options.
See next page for figure.

The overhead of each optimiser is minimal, and is unlikely to affect the time
taken to train the network.

The choice of optimiser is provided when compiling the model, via the
\texttt{optimizer} argument.
If no optimiser if provided, the default option, which was RMSProp, will be
used.
If a string is provided, TensorFlow will use the corresponding optimiser with
default parameters.
If an optimiser instance is provided, that instance will be used.

One may also define their own optimiser class.

\rfig{\input{DiagramBHTFVary.tex}}{%
    Graph of training loss against time for the Boston housing data using
    various optimisers.
}

%2000 epochs

%SGD [5,5]
%initial loss 0.228166446089744570
%  final loss 0.042430859059095380
%initial vali 0.214338675141334530
%  final vali 0.046398963779211044
% vali / loss 1.093519311

%SGD [3,3]
%initial loss 0.285416126251220700
%  final loss 0.033157549798488620
%initial vali 0.329692572355270400
%  final vali 0.045487113296985626
% vali / loss

%SGD [3]
%initial loss 0.494845181703567500
%  final loss 0.041440241038799286
%initial vali 0.568243682384491000
%  final vali 0.048694450408220290

%Adams [5,5]
%initial loss 0.425099462270736700
%  final loss 0.014745806343853474
%initial vali 0.409027785062789900
%  final vali 0.044336102902889250

\subsection{Activation Functions}

So far, only $\tanh$ and linear activation functions have been considered.
TensorFlow provides a total of eleven activation functions:
\begin{itemize}
    \item\texttt{linear}, no activation function applied,
        \begin{align*}
            \text{linear}(x) &= x;
        \end{align*}

    \item\texttt{relu}, rectified linear unit,
        \begin{align*}
            \text{relu}(x) &= \begin{cases}
                m, & x > m\\
                \alpha x, & x < 0\\
                x, & \text{otherwise}
            \end{cases},
        \end{align*}
        where $\alpha = 0$ and $m = \infty$ by default;

    \item\texttt{exponential},
        \begin{align*}
            \text{exponential}(x) &= e^x;
        \end{align*}

    \item\texttt{elu}, exponential linear unit,
        \begin{align*}
            \text{elu}(x) &= \begin{cases}
                \alpha (e^x - 1) & x < 0\\
                x, & \text{otherwise}
            \end{cases},
        \end{align*}
        where $\alpha = 1$ by default;

    \item\texttt{selu}, special case of elu with an additional scaling factor
        and fixed parameters;

    \item\texttt{sigmoid},
        \begin{align*}
            \text{sigmoid}(x) &= \frac{1}{1 + e^{-x}};
        \end{align*}

    \item\texttt{hard\_sigmoid}, approximation of sigmoid using three linear
        segments,
        \begin{align*}
            \text{hard-sigmoid}(x) &= \begin{cases}
                0, & x < -2.5\\
                1, & x > 2.5\\
                0.2x + 0.5, & \text{otherwise}
            \end{cases};
        \end{align*}

    \item\texttt{tanh},
        \begin{align*}
            \tanh(x) &= \frac{e^x - e^{-x}}{e^x + e^{-x}};
        \end{align*}

    \item\texttt{softsign}, smoothed sign function,
        \begin{align*}
            \text{softsign}(x) &= \frac{x}{|x| + 1};
        \end{align*}

    \item\texttt{softplus}, smoothed $\max(x,0)$ function,
        \begin{align*}
            \text{softplus}(x) &= \ln(e^x+1);
        \end{align*}

    \item\texttt{softmax}, normalised exponentials,
        \begin{align*}
            y_i &= \frac{e^{x_i}}{\sum_j e^{x_j}},
        \end{align*}
        where $x_i$ is the net value into softmax neuron $i$.
\end{itemize}
With the exception of linear, all of these functions are discontinuous and/or
bounded.
These properties are useful for activation functions as they allow groups of
neurons to approximate heavyside functions, which is necessary for decision
making.
Note that the tanh and sigmoid functions are essentially equivalent, as
\begin{align*}
    \tanh(x) &= 2\text{sigmoid}(2x) - 1,
\end{align*}
and that such a transformation is possible on any, non-output layer.

The softmax function is typically used on the output layer of classification
models, with the results representing a probability distribution.

One may also use a custom activation function by using the TensorFlow math
functions.



\subsection{Layer Types and Character Recognition}

TensorFlow provides a large number of layer types that can easily be added to a
model, as well as tools for defining custom layers.
The most commonly use ones are:
\begin{itemize}
    \item AveragePooling,
    \item MaxPooling,
    \item and Convolution, which are available in 1D, 2D, and 3D variants;

    \item Dense, each neuron connects to all neurons from the previous layer;
    \item Dropout, one to one mapping of the previous layer, setting a random
        selection of a fix proportion, to zero during training;
    \item Flatten, takes the previous layer and makes it into a 1D array of
        neurons;

    \item SimpleRNN, fully connected recurrent neural network layer;
    \item GRU, layer of gated recurrent units;
    \item LSTM, layer of long short-term memory units.
\end{itemize}
These layers can be roughly categorised into three groups:
spacial layers, which use neuron locality to encode position, namely pooling and
convolutional layers;
simple layers, which act irrespective of input shape, such as dense, dropout,
and flatten;
and recurent layers, which have recurrent connections, such as SimpleRNN, GRU,
and LSTM.
There is also ConvLSTM2D layer, which has convolutional connections on both
input and recurrent transformations, which may be classed as spacial and
recurrent.

It should be noted that the $N$ dimensional pooling and convolutional layers
expect $N+1$ dimensional tensors, where the last dimension is the channel of the
input.
For example: to use $w \times h$ greyscale images as the network input, the
inputs must be reshaped into a $w \times h \times 1$ tensor.

To test the spacial layers, an image classification problem was considered.
The MNIST dataset provides 70000, $28\times28$ images of handwritten numbers,
and a set of corresponding labels.

The dataset was split into training and validation sets of 60000 and 10000
samples respectively.

For all of the following networks, the
\texttt{sparse\_categorical\_crossentropy} loss function was used, which uses
the data label as the index of the neuron that should be most active.

First, a network using: a flatten layer; three dense relu layers of 128, 64, and
32 neurons respectively; and a dense softmax output; was considered.
From preliminary testing, the relu function was the most optimal choice for the
hidden layers.

With a total of 111,146 parameters, the network was trained for 350 epochs.
The final training and validation loss values were 0.0264 and 0.0956
respectively.
The trained network correctly identified 58674/60000 training samples, and
8923/10000 validation samples, giving an error of 2.21\% and 10.77\%
respectively.
With a loss ratio of $\sim3.6$, the network is very likely to be over-fitting,
which is further supported by difference in error rates.
\newpage
~
\vfill
\rfig[2em]{\input{DiagramCRResults.tex}}{%
    Hand written numbers from the MNIST validation set, with the prediction and
    certainty values, for the dense, non-dropout network.
}
\vfill
~
\newpage
In order to prevent over-fitting, dropout layers were added after each relu
layer.
By setting random neurons to 0, the dropout layers prevent neurons from forming
codependence \citep{Hinton:2012:Dropout}.
All three dropout layers used a rate of 0.2, meaning that a fifth of the neurons
were set to zero during training.

After training the new network for 350 epochs, the training and validation loss
values were 0.1055 and 0.0941 respectively.
It should be noted that the training loss is based of the network with dropout
enabled, whereas the validation loss is with dropout disabled.
The trained network correctly identified 57157/60000 training samples, and
9024/10000 validation samples, with dropout disabled in both cases, giving an
error of 4.74\% and 9.76\% respectively.

Although the new network has clearly underperformed on the training data in
comparison to the previous model, the amount of over-fitting has been reduced,
and the network performance on the training data is much more representative of
the actual network performance.
The difference in validation performance measures suggest an improvement, as
both loss and error values are reduced by the new model.

As mentioned in Section \ref{subsec:history:conv}, densely connected networks
struggle to correctly identify shifted inputs, which makes them less suited to
image recognition tasks.
As such a network using: three, $7\times7$ convolutional relu layers with 4, 6,
and 10 filters respectively; a flatten layer; and a dense softmax output; was
also considered.
Due to the number of neurons in the network and memory constraints, full-batch
processing was not possible; for all of the following the networks a batch size
of 600 was used.

With a total of 14,342 parameters, the network was trained 20 epochs.
The final training and validation loss values were 0.0557 and 0.0566
respectively.
The trained network correctly identified 56166/60000 training samples, and
9313/10000 validation samples, giving an error of 6.39\% and 6.87\%
respectively.

Comparing the convolutional network against both dense networks, the
convolutional network may have reduced performance on the training samples, but
is significantly better on the validation samples.
Further more, with a loss ratio of $\sim1.02$, the network does not appear to be
over-fitting.

As with the dense network, dropout layers were then added after each
convolutional layer.
After training the new network for 20 epochs, the training and validation loss
values were 0.0593 and 0.0408 respectively.
The trained network correctly identified 57493/60000 training samples and
9499/10000 validation samples, giving an error of 4.18\% and 5.01\%
respectively.

Even though over-fitting was not a problem in the non-dropout convolutional network, the use of dropout layers has improved the performance of the network

\rfig{\begin{tabular}{r|cc|cc}
        & \multicolumn{2}{c|}{Dense} & \multicolumn{2}{c}{Convolutional}\\
        \hline
        Dropout?         & No      & Yes     & No      & Yes   \\
        \hline
        Training Loss    &  0.0264 &  0.1055 &  0.0557 &  0.0593\\
        Training Error   &  2.21\% &  4.74\% &  6.39\% &  4.18\%\\
        \hline
        Validation Loss  &  0.0956 &  0.0941 &  0.0566 &  0.0408\\
        Validation Error & 10.77\% &  9.76\% &  6.87\% &  5.01\%\\
    \end{tabular}}{%
    Table summarising the network loss and error values for all of the character
    recognition networks.
}



%own theory of composition
% functions f and g are compositionally similar if:
%  for small n:
%   \sum_{i=1}^{n} A_if(a_ix+b_i) + B = g(x)
%own theory of versatility
% function f is versatile if:
%  \sum_{i}^{\infty} A_if(a_ix+b_i) \approx \delta(\alpha)
% theory:
%  all discontinuous functions are versatile
%desirable properties of activation functions
% no vanishing/zero-gradient areas
% versatile
%  either discontinuous
%  or bounded

%\subsection{Linear Regression}
%
%\rfig{\input{DiagramRegLinear.tex}}{%
%    Graph showing the data points and network's corresponding line of best fit.
%}
%initial loss 0.62278741598129270000
%  final loss 0.00022495193115901202

%\subsection{Chaos Prediction}


