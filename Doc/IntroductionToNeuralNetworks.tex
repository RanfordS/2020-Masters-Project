\chapter{Introduction to Neural Networks}

\TODO{Chapter: Introduction}

\section{Biological Neurons}

Biological neurons are electrically excitable cells that are found in almost all
animals.
These neurons can transmit and receive electrical signals to one another via
synaptic connections, which maybe either excitatory or inhibitory.
Any given neuron will be either active or inactive depending on whether or not
its input exceeds a threshold.

\begin{center}
    \input{DiagramNeuron.tex}
    \captionof{figure}{Diagram of a biological neuron.}
\end{center}

Signals are received by the neuron via connections to dendrites and soma.
If the threshold is met, electrical signals are sent along the axon to the
terminal, where it is connected to other neurons, or to a controllable cell such
as a neuromuscular junction.

\TODO{Section: Biological Neurons}

\section{Artificial Intelligence}

The idea of artificial beings capable of human intelligence can be traced back
to mythical stories from ancient Greece.
One such story was that of a mythical automaton called Talos, who circled an
island's shores to protect it from pirates and other invaders.

In the $19^\text{th}$ century, other notions of artificial intelligence were
explored by fiction in stories such as Mary Shelley's ``Frankenstein'', and
Karel \v{C}apek's ``R.U.R.''.
Some of the fictional writings of the $20^\text{th}$ century further continued
exploring the concept in novels such as Isaac Asimov's ``I, Robot''.

Academic research into artificial intelligence began around the 1940's,
primarily due to findings in neurological research at the time.
The first explorations of artificial neural networks was done by
\cite{McCulloch:1943:Logical}, who investigated how simple logic functions
might be performed by idealised networks.
The neurons within these networks operated using some basic logic rules, applied
to a discrete time system which, can be summarised using the expression
\begin{align*}
    N(t) &= (E_1(t-1) \vee E_2(t-1) \vee \dots)
        \wedge \neg(I_1(t-1) \vee I_2(t-1) \vee \dots),
\end{align*}
where $N(t)$ is the state of a neuron at time $t$, and $E_i(t-1)$ and $I_i(t-1)$
are the states of the excitatory and inhibitory connections from the previous
time step respectively.
The result is such that the neuron will only be active if at least one
excitatory connection is active and all inhibitory connections are inactive.
The versatility of this definition is demonstrated in the following examples.
\begin{center}
    \begin{tabular}{ccc}
        SR Flip-flop & & AND Gate\\
        \input{DiagramMcculSRFlipFlop.tex} & & \input{DiagramMcculAND.tex}\\
        $\displaystyle N_M(t+1) = (N_S(t) \vee N_M(t)) \wedge\neg N_R(t)$ &
        &
        $\displaystyle N_R(t+2) = N_A(t) \vee N_B(t)$\\
    \end{tabular}
    \parbox{0.9\textwidth}{%
    \captionof{figure}{Two common logic circuits using McCullochs neurons
    where the arrows and circles indicate excitatory and inhibitory
    connections respectively.}}
\end{center}
While this model provided insight into the mechanisms by which neurons operate,
the structure was static, and incapable of learning.

McCulloch's work was later cited by psychologist \cite{Hebb:1949:Organization},
who proposed that the structure of biological neural networks was dynamic, and
that frequently repeated stimuli caused gradual development.
At the scale of neuron, it was theorised that if one neuron successfully
excited another, the connection between them would strengthen, hence
increasing the likelihood that the former would be able to excite the latter
again in the future.
His theory was supported by research conducted by himself and others that showed
that intelligence-test performance in mature patients was often unaffected by
brain operations that would have prevented development in younger patients,
which suggested that learnt stimuli are processed differently to unknown
stimuli.
This hypothesis became known as Hebbian learning.

Computer simulations applying this theory to a small network were done by
\cite{Farley:1954:Simulation}.
The actions of the network were compared to that of a servo system which must
counteract any displacements so as to maintain a steady position.
The network was trained using a set of input patterns, which were subject to
non-linear transformations.
Similar to the Hebbian theory, when the network produces the correct responses,
the active connections are strengthened.
Although the results were of little neurophysiological significance, the results
were of great use for demonstrating computational simulations, which were
considerably slower at the time.

\TODO{Section: Artificial Intelligence}

\subsection{Perceptrons}

The idea of the perceptron was originally conceived by
\cite{Rosenblatt:1958:Perceptron}, to represent a simplified model of
intelligent systems free from particularities of biological organisms, whilst
maintaining some of their fundamental properties.

The perceptron was built as a dedicated machine that consisted of a number of
photovoltaic cells, analogous to a retina, that feed into an``association
area''.
This association area contains a number of cells that each calculate a weighted
sum of the receptor values and output a signal if it exceeds a threshold.
Expressed mathematically, the output of a given association cell is given by
\begin{align*}
    A_i &= \begin{cases}
        1, & \sum_j w_{i,j}x_j > \theta\\
        0, & \text{otherwise}
    \end{cases},
\end{align*}
where $x_j$ is the value from the $j^\text{th}$ photovoltaic cell, $w_{i,j}$
is the weight of the connection between association cell $i$ and photovoltaic
cell $j$, and $\theta$ is the threshold.
These value weights were implemented using variable resistance wires that the
perceptron could adjust automatically.
The outputs from the association area are then connected to response cells,
which operate in a similar fashion to the association cells.
The activation of these response cells are the outputs of the perceptron, and
indicated the classification of the input.

Similar to \cite{Farley:1954:Simulation}, the method by which the perceptron
adjusted it weights was also based on which cells were active, and whether the
correct output was produced; except that the perceptron was also able to
``penalise'' weights when an incorrect result was outputted.

This machine was initially trained to reliably identify three different shapes:
a square, a circle, and a triangle; and did so with a better than chance
probability.
When attempting to use the perceptron for more complicated tasks, such as
character recognition, it failed to produce better than chance results.

After a decade of unsuccessful real world application attempts, a book titled
``\citetitle{Minsky:1969:Perceptrons}'' by \cite{Minsky:1969:Perceptrons}, was
released.
The book provided a rigorous mathematical analysis of the model, the results
showed that single layered, simple linear perceptron networks could not
calculate XOR predicates.
A 2017 reissue of the book contained a foreword by L\'eon Bottou, who wrote
``Their rigorous work and brilliant technique does not make the perceptron look
very good...''

Following the book's release, perceptron research effectively halted for 15
years until the first successful uses of multilayer networks by
\cite{McClelland:1986:Parallel}, which also served as a departure from the
neuron outputs being boolean values.
The multilayered structure of this new model allowed it to calculate the XOR
predicates that the single layer perceptrons could not.

The output of the units within these networks were defined by
\begin{align*}
    \Rvec{a}(t+1) &= \Rvec{F}(\Rvec{a}(t),\Rvec{net}_1(t),\Rvec{net}_2(t),...),
\end{align*}
where $\Rvec{net}_i$ is the $i^\text{th}$ propagation rule applied to the
inputs, $\Rvec{F}$ is the activation function, and $\Rvec{a}(t)$ is the
activation of the units at time step $t$.
The model usually uses a simplified version which can be summarised as
\begin{align*}
    a_i &= F\left(\sum_j w_{i,j}o_j\right),
\end{align*}
where $o_j$ is the output of unit $j$.
Hebbian learning could be performed the network by using iterative methods,
the most simple of which was given by
\begin{align*}
    \Delta w_{i,j} &= \eta\,a_i o_j,
\end{align*}
where $\eta$ is the learning rate, which is a constant.

\TODO{Subsection: Perceptrons}

\subsection{Backpropagation}

In order for a neural network to learn, it must undergo some form of
optimisation process.
For the perceptron, this process was one of positive and negative reinforcement.

In the field of control theory, an optimisation method known as gradient descent
was developed by \cite{Kelley:1960:Gradient}, in which a given function of the
system is either maximised or minimised.
This is achieved by taking partial derivatives of the function with respect to
each parameter, which gives an approximation of how the function value will
change as the parameter changes.
By evaluating the partial derivatives, multiplying them by a constant, and
adding them to their respective parameters, the parameter values can be updated.
Using these new parameter values, one can expect to improve the function value.
This can be written as
\begin{align*}
    w_i' &= w_i + \eta\Rpdiff{f}{w_i}(\Rvec{x}),
\end{align*}
where $f(\Rvec{x})$ is the function to be optimised, $w_i$ is a parameter of
$f$, $w_i'$ is the updated parameter, and $\eta$ is ascent/descent parameter.
Positive $\eta$ values will maximise the function value, where as negative
values will minimise it.
The magnitude of $\eta$ determines the rate at which the method will attempt
change the parameters: if the value is too large, the method will overshoot the
optimal values; if the value is too small, the method will be too slow to
converge.

When this method was applied to neural networks, researchers sometimes
encountered an issue now known as the vanishing gradient problem.
A computer program will typically calculate the gradient via repeat applications
of chain rule; if there are many small terms, the gradient will tend to zero,
and the learning rate of the network will be minimal.

One of the methods that overcame this problem was developed by
\cite{Schmidhuber:1992:Compression}, where each layer of the network was
pre-trained to predict the next input from previous inputs.
Once each layer had been pre-trained, the network was then fine tuned using
backpropagation.
The method also provided a way of calculating which inputs were least expected,
so that more training time could be devoted to learning them.

Since then, computational power has significantly increased, and the slow
convergence caused by the vanishing gradient problem is less significant.
Further more, backpropagation and a simple variant the model outlined by
\cite{McClelland:1986:Parallel}, have become the standard for neural networks.
Namely
\begin{align*}
    x_i &= \phi\left(b_i + \sum_j w_{i,j} x_i\right),\\
\end{align*}
where $x_i$ is the output of neuron $i$, $w_{i,j}$ is the weight of connection
from $j$ to $j$, $b_i$ is the input bias of $i$, and $\phi$ is the activation
function.

\TODO{Subsection: Backpropagation}

\section{Types of Artificial Neurons}

The preceding discussion has been focused on densely connected neural
networks, where each neuron in a layer is connected to every neuron in the
previous, but it is important to note, that many other neural network
architectures are often used together, and maybe more suitable under certain
contexts.

\TODO{Section: Types of Neurons}

\subsection{Convolutional Neural Networks}

Often used in image processing, a Convolutional Neural Network (CNN) layer
contains neurons that sample different regions of the previous layer using the
same weight scheme.
The 1D case may be expressed mathematically as
\begin{align*}
    y_i &= \phi\left(b + \sum_{j=0}^{S} w_j x_{i+j}\right),
\end{align*}
where $S$ is the kernel size.
\begin{center}
    \input{DiagramCNN.tex}
    \captionof{figure}{Example of a 1D CNN.}
\end{center}
CNNs have the advantage of encoding spacial information implicitly, which makes
them well suited for image processing.

\subsection{RNN}
\subsection{LSTM}
